{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "## Homework 1: Spark RDDs, Spark SQL and Hive\n",
    "### Milan Vojnovic and Christine Yuen, edited by Simon Schoeller LT 2019\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "**Deadline**: February 27, 2019, 5pm London time\n",
    "\n",
    "**Datasets**: All the required datasets are available for download from here:\n",
    "\n",
    "https://www.dropbox.com/sh/89xbpcjl4oq0j4w/AACrbtUzm3oCW1OcpL7BasRfa?dl=0\n",
    "in the respective sub-directories.\n",
    "\n",
    "Please make sure that you document your work appropriately. If you get stuck somewhere, make sure to give the other parts a try.\n",
    "\n",
    "\n",
    "## A. Spark RDDs (30 points)\n",
    "\n",
    "We continue to analyse the dblp dataset available in the file `author-large.txt`. This time, we want to find the top 10 pairs of authors who published the largest number of papers together (with possible other collaborators). For example, if authors $a$, $b$ and $c$ published a paper with title $t$, then this contributes one joint publication for each author pair ($a$,$b$), ($b$,$c$) and ($a$,$c$). Use the first column of the input data for the author names and use the third column of the input data for the publication title. \n",
    "\n",
    "You need to solve this task by using RDD operations in pyspark like those in `pyspark_rdd.ipynb` in week 3 of the course and the [Spark RDD documentation]( http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD). You can run your code on your laptop or GCP. Please make sure to give us all your code and document what you have done outside of the notebook, for example using terminal in- and output or screenshots. *Please make sure to delete you dataproc clusters and buckets afterwards.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Spark SQL (30 points)\n",
    "\n",
    "Do the same as in problem A, but this time use the Spark SQL API, which we covered in week 4. You may find useful to consult 'Querying with Spark SQL' in `spark-dataframe-sql.ipynb` of week 4 class and the [Spark SQL documentation](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Hive (40 points)\n",
    "\n",
    "In this part we are going to use the Yelp data available in the following JSON file `Yelp/yelp_academic_dataset_user.json`. You may complete this task by using either Hive installed on your laptop or using Hive on Google Cloud Platform. Please complete the following steps:\n",
    "\n",
    "_(Here, it is particularly important that you find a suitable way to document your work appropriately.)_\n",
    "\n",
    "### 1. Load data into a Hive table\n",
    "\n",
    "Create a Hive table and load the input data into this table.\n",
    "\n",
    "Please describe any commmands that you run in a command line interface, provide all the code that you wrote and ran. For example, this may include any commands run in a terminal, Hive script files (\\*.sql), and screenshots (if, for example, you used Google Cloud Platform through the browser interface). See the class examples for references.\n",
    "\n",
    "Note:\n",
    "* The dataset is in JSON format whereas in the class the datasets were in XML or TXT format. You will need to figure out (look up) how to load data from a JSON file to a Hive table. \n",
    "* You will need to infer the schema by looking at the data. \n",
    "\n",
    "Hints: \n",
    "\n",
    "* Some of the columns are of array type. For example, you should use array&lt;STRING&gt; for the friends column.\n",
    "* The size of the dataset is large (about 1GB). You may want to create a smaller dataset first and work with this smaller dataset until you develop and test your code, and then apply it on the original dataset.\n",
    "\n",
    "\n",
    "### 2. Simple queries\n",
    "\n",
    "Having created the Hive table and loaded the data into it, write and execute queries to:\n",
    "\n",
    "i. retrieve the schema;\n",
    "\n",
    "ii. show the number of rows in the table;\n",
    "\n",
    "iii. select top 10 users who have provided the largest number of reviews (the output should consist of the user name and the number of reviews of the users).\n",
    "\n",
    "For all the queries, please show both the commands you used and the output. You may copy and paste the commands that you run and the outputs, or provide screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

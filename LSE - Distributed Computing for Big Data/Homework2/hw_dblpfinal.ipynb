{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "## Homework PART 3\n",
    "### Milan Vojnovic, Christine Yuen, Simon Schoeller LT 2019\n",
    "---\n",
    "\n",
    "## P3: Topic Modelling\n",
    "\n",
    "In this homework problem, you are asked to perform a semantic analysis of the DBLP author publications dataset `dblp/author_large.txt` that you have already encountered before. *You may use GCP or your own computer. Please document your steps. The necessary initialisation actions for `nltk` are provided as part of week 8's class material.*\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /usr/local/share/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark mllib.li\n",
    "\n",
    "nltk.download('all')\n",
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A** Use Latent Dirichlet Allocation (LDA) to cluster publications by using words in their titles and represent each publication by 10 topics. Please follow these steps:\n",
    "\n",
    "**A.1 (max points 0)** Convert titles to tokens by:\n",
    "   * Tokenizing words in the title of each publication\n",
    "   * Removing stop words using the nltk package\n",
    "   * Removing puctuations, numbers or other symbols\n",
    "   * Lemmatizing tokens\n",
    "\n",
    "Note that you may skip some of these editing steps or add some additional steps to edit the tokens, but if you do this provide a justification for it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_file = sc.\\\n",
    "    textFile(\n",
    "        \"gs://anyabucket01apr2019/author-large.txt\", \n",
    "        4)\n",
    "\n",
    "data_from_file_conv = data_from_file.map(lambda row: np.array(row.strip().split(\"\\t\")))\n",
    "paperlist = data_from_file_conv.map(lambda row: (row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Object SQL - A Language for the Design and Implementation of Object Databases.', 'Object SQL - A Language for the Design and Implementation of Object Databases.', 'Object SQL - A Language for the Design and Implementation of Object Databases.']\n"
     ]
    }
   ],
   "source": [
    "print(paperlist.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def get_tokens(line):\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuations from each word\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatizing the words, see https://en.wikipedia.org/wiki/Lemmatisation\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    return (words)\n",
    "\n",
    "titles_rdd = paperlist.map(lambda line: (1, get_tokens(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, ['object', 'sql', 'language', 'design', 'implementation', 'object', 'database']), (1, ['object', 'sql', 'language', 'design', 'implementation', 'object', 'database']), (1, ['object', 'sql', 'language', 'design', 'implementation', 'object', 'database'])]\n"
     ]
    }
   ],
   "source": [
    "print(titles_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dummy=1, words=['object', 'sql', 'language', 'design', 'implementation', 'object', 'database']),\n",
       " Row(dummy=1, words=['object', 'sql', 'language', 'design', 'implementation', 'object', 'database']),\n",
       " Row(dummy=1, words=['object', 'sql', 'language', 'design', 'implementation', 'object', 'database'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_df = spark.createDataFrame(titles_rdd, [\"dummy\",\"words\"])\n",
    "titles_df.cache()\n",
    "titles_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.2 (max points 5)** Convert tokens into sparse vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|dummy|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|[object, sql, lan...|(163793,[9,38,41,...|\n",
      "|    1|[object, sql, lan...|(163793,[9,38,41,...|\n",
      "|    1|[object, sql, lan...|(163793,[9,38,41,...|\n",
      "|    1|[object, sql, lan...|(163793,[9,38,41,...|\n",
      "|    1|[object, sql, lan...|(163793,[9,38,41,...|\n",
      "|    1|[object, sql, lan...|(163793,[9,38,41,...|\n",
      "|    1|[oql, c, extendin...|(163793,[41,80,64...|\n",
      "|    1|[transaction, man...|(163793,[0,23,474...|\n",
      "|    1|[transaction, man...|(163793,[0,23,474...|\n",
      "|    1|[transaction, man...|(163793,[0,23,474...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2)\n",
    "\n",
    "cv_model = cv.fit(titles_df)\n",
    "\n",
    "titles_df_w_features = cv_model.transform(titles_df)\n",
    "titles_df_w_features.cache()\n",
    "titles_df_w_features.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.3 (max points 5)** Use LDA to find out 10 topics for each publication and represent each topic with the first few most relevant words. Note that you can choose to use different number of topics rather than 10. Again if you do so, please provide a justification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda = LDA(k=10, maxIter=5)\n",
    "\n",
    "lda_model = lda.fit(titles_df_w_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+----------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices           |termWeights                                                                                                        |\n",
      "+-----+----------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[11, 1, 4, 3, 6]      |[0.013312622643395916, 0.010513942737104164, 0.007720714929877752, 0.0071018861541697394, 0.005858425931166294]    |\n",
      "|1    |[18, 25, 1, 4, 10]    |[0.0027081270284537843, 0.00266206186588479, 0.0026542379276941544, 0.0024924058373042767, 0.0019553111068304945]  |\n",
      "|2    |[8, 2, 6, 9, 29]      |[0.0027167173338182764, 0.0025410254050279774, 0.0019521300667035147, 0.0018491563838020154, 0.0018304669485381406]|\n",
      "|3    |[0, 2, 3, 1, 6]       |[0.018232262204466033, 0.0173877552383729, 0.012116399098228486, 0.010186855343657064, 0.008236464541922799]       |\n",
      "|4    |[10, 0, 24, 47, 4]    |[0.0036969540922358723, 0.0036458757648786607, 0.0035199749035410197, 0.0030979352966448563, 0.002957273265847965] |\n",
      "|5    |[0, 9, 1, 38, 6]      |[0.007750167061846985, 0.0023654929245990634, 0.0018546204392893837, 0.001500080179884901, 0.0014984696782103612]  |\n",
      "|6    |[0, 1, 5, 10, 28]     |[0.01762377637667326, 0.016253789723212432, 0.009038870295993331, 0.006621243435313042, 0.006586303718942311]      |\n",
      "|7    |[2, 10, 18, 5, 16]    |[0.002512511695984118, 0.002327101851455213, 0.0014250905244030158, 0.0013758143550472422, 0.001347582895694536]   |\n",
      "|8    |[93, 0, 225, 234, 249]|[0.011280672579819179, 0.0059145425367665184, 0.005553590671281596, 0.0052258802452938465, 0.004676911878712697]   |\n",
      "|9    |[59, 4, 7, 39, 8]     |[0.009312596334668886, 0.004960406982016661, 0.0036728243439072632, 0.0036233196325321346, 0.003541442976424421]   |\n",
      "+-----+----------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "['image' 'using' 'model' 'based' 'data']\n",
      "['control' 'mobile' 'using' 'model' 'approach']\n",
      "['application' 'network' 'data' 'design' 'study']\n",
      "['system' 'network' 'based' 'using' 'data']\n",
      "['approach' 'system' 'software' 'language' 'model']\n",
      "['system' 'design' 'using' 'database' 'data']\n",
      "['system' 'using' 'algorithm' 'approach' 'parallel']\n",
      "['network' 'approach' 'control' 'algorithm' 'web']\n",
      "['de' 'system' 'und' 'von' 'der']\n",
      "['logic' 'model' 'analysis' 'process' 'application']\n"
     ]
    }
   ],
   "source": [
    "# Describe topics\n",
    "topics = lda_model.describeTopics(5)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the results\n",
    "import numpy as np\n",
    "topic_i = topics.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(cv_model.vocabulary)[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.4 (max points 0)** Comment the obtained results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the words tend to be very technical based.  Words like system and network are some of the most popular, and pop up in multiple topics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B** \n",
    "\n",
    "**B.1-B.4** Address each question as in part A, but with each *document* representing all publication tiles of a specific author. For example, if an author $Y$ wrote \"introduction to databases\" and \"database design\", then the *document* for the author $Y$ will be \"introduction to database database design\". \n",
    "\n",
    "**(B.1 (max points 0); B.2 (max points 5); B.3 (max points 5); B.4 (max points 0))**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abraham Silberschatz',\n",
       "  [\"Transaction Management in Multidatabase Systems. Data Models. The Storage and Retrieval of Continuous Media Data. Serializability in multi-level monitor environments. Overview of multidatabase transaction management. On the Storage and Retrieval of Continuous Media Data. Adaptive Commitment for Distributed Real-Time Transactions. Performance Analysis of Storage Systems. Efficient Global Transaction Management in Multidatabase Systems. Efficiently Monitoring Bandwidth and Latency in IP Networks. Topology Discovery in Heterogeneous IP Networks. Optimal ISP subscription for Internet multihoming: algorithm design and implication analysis. A Biased Non-Two-Phase Locking Protocol. On Subjective Measures of Interestingness in Knowledge Discovery. Kernel Support for Recoverable-Persistent Virtual Memory. Move-to-Rear List Scheduling: A New Scheduling Algorithm for Providing QoS Guarantees. System-Wide Multiresolution. Distributed Multi-Level Recovery in Main-Memory Databases. Non-Serializable Executions in Heterogeneous Distributed Database Systems. A Theory of Relaxed Atomicity (Extended Abstract). A Multi-Version Concurrency Scheme With No Rollbacks. Concurrency Control in Graph Protocols by Using Edge Locks. Efficient and Acurate Cost Models for Parallel Query Optimization. Throughput-Competitive Admission Control for Continuous Media Databases. View Maintenance Issues for the Chronicle Data Model. An Axiomatic Approach to Deciding Query Safety in Deductive Databases. Ensuring Transaction Atomicity in Multidatabase Systems. Compatibility and Commutativity in Non-two-phase Locking Protocols. Multimedia Support for Databases. Safety of Recursive Horn Clauses With Infinite Relations. Strict Histories in Object-Based Database Systems. On Correctness of Non-serializable Executions. Data-value Partitioning and Virtual Messages. Annotations for Distributed Programming in Logic. On Periodic Resource Scheduling for Continuous Media Databases. Strong Recoverability in Multidatabase Systems. Storage and Retrieval of Multimedia Objects. Relaxing Serializability in Multidatabase Systems. Algorithms for provisioning virtual private networks in the hose model. P4p: provider portal for applications. On the Modeling and Performance Characteristics of a Serpentine Tape Drive. DataBlitz Storage Manager: Main Memory Database Performance for Critical Applications. Update Propagation Protocols For Replicated Databases. Multidatabase Update Issues. Reliable Transaction Management in a Multidatabase System. Bifocal Sampling for Skew-Resistant Join Size Estimation. A Framework for the Parallel Processing of Datalog Queries. Random I/O Scheduling in Online Tertiary Storage Systems. Deadlock Removal Using Partial Rollback in Database Systems. An Optimistic Commit Protocol for Distributed Transaction Management. The Concurrency Control Problem in Multidatabases: Characteristics and Solutions. Fault-tolerant Architectures for Continuous Media Servers. Distributed Processing of Logic Programs. Building appliances out of components using Pebble. Scalable and Non-Intrusive Load Sharing in Owner-Based Distributed Systems. Retrofitting Quality of Service into a Time-Sharing Operating System. Signaled Receiver Processing. The Pebble Component-Based Operating System. Lightweight Security Primitives for E-Commerce. DataBlitz: A High Performance Main-Memory Storage Manager. Garbage Collection in Object Oriented Databases Using Transactional Cyclic Reference Counting. A Database System for Real-Time Event Aggregation in Telecommunication. Information, Communication, and Money: For What Can We Charge and How Can We Meter It? Logical and Physical Versioning in Main Memory Databases. Obtaining Progressive Protocols for a Simple Multiversion Database Model. Modeling Skewed Distribution Using Multifractals and the `80-20' Law. Resource Scheduling in Enhanced Pay-Per-View Continuous Media Databases. A Theory of Correct Locking Protocols for Database Systems Dal: A High Performance Main Memory Storage Manager. Recovering from Main-Memory Lapses. An Efficient Deadlock Removal Scheme for Non-Two-Phase Locking Protocols. Non-Two-Phase Locking Protocols with Shared and Exclusive Locks. A Formal Approach to Recovery by Compensating Transactions. Triggered Real-Time Databases with Consistency Constraints. Challenges for Global Information Systems. A Low-Cost Storage Server for Movie on Demand Databases. Scientific Journals: Extinction or Explosion? (Panel). The MR Diagram - A Model for Conceptual Database Design. On the Discovery of Interesting Patterns in Association Rules. A Multi-Resolution Relational Data Model. PANACE: A System That Uses Database Technology to Manage Networks. Safety of Recursive Horn Clauses With Function Symbols. Controlling Concurrency Using Locking Protocols (Preliminary Report) Eliminating Cascading Rollback in Structured Database. On the Storage and Retrieval of Continous Media Data (Abstract). Design and Evaluation of Redistribution Strategies for Wide-Area Commodity Distribution. A Failure Tolerant Centralized Mutual Exclusion Algorithm. A Transaction Model for Multidatabase Systems. Exploiting Transaction Semantics in Multidatabase Systems. Using Codewords to Protect Database Data from a Class of Software Errors. New and Forgotten Dreams in Database Research (Panel). An Analysis Technique for Transitive Closure Algorithms: A Statistical Approach. Scheduling and Data Replication to Improve Tape Jukebox Performance. Unilateral Commit: A New Paradigm for Reliable Distributed Transaction Processing. Periodic Retrieval of Videos from Disk Arrays. Cyclic Association Rules. Disk Scheduling with Quality of Service Guarantees. A Framework for the Storage and Retrieval of Continous Media Data. Buffer Replacement Algorithms for Multimedia Storage Systems. Disk Striping in Video Server Environments. Demand Paging for Video-on-Demand Servers. On Mapping Homogeneous Graphs on a Linear Array-Processor Model. Extending Concurrent Pascal to Allow Dynamic Resource Management (Abstract).\"]),\n",
       " ('Eric N. Hanson',\n",
       "  ['Active Database Systems. The Ariel Project Ariel. Testing for termination of asynchronous parallel computations. An Analysis of Rule Indexing Implementations in Data Base Systems. Heuristic Search in Data Base Systems. Experiences in DBMS Implementation Using an Object-Oriented Persistent Programming Language and a Database Toolkit. An Introduction to the TriggerMan Asynchronous Trigger Processor. A Performance Analysis of View Materialization Strategies. Processing Queries Against Database Procedures: A Performance Analysis. Rule Condition Testing and Action Execution in Ariel. A Predicate Matching Algorithm for Database Rule Systems. Quel as a Data Type. The Interval Skip List: A Data Structure for Finding All Intervals that Overlap a Point. Scalable Trigger Processing. The Design of the Postgres Rules System. A Performance Comparison of the Rete and TREAT Algorithms for Testing Database Rule Conditions.']),\n",
       " ('Jennifer Widom',\n",
       "  ['Active Database Systems. The Starburst Rule System Introduction to Active Database Systems Standards and Commercial Systems Applications of Active Databases Conclusions and Future Directions Expressiveness Bounds for Completness in Trace-Based Network Proof Systems. Trio-One: Layering Uncertainty and Lineage on a Conventional DBMS (Demo). Query Processing, Approximation, and Resource Management in a Data Stream Management System. Trio: A System for Integrated Management of Data, Accuracy, and Lineage. Research Problems in Data Warehousing. Integrity Constraint Checking in Federated Databases. Validating Constraints with Partial Information: Research Overview. CQL: A Language for Continuous Queries over Streams and Relations. Ozone: Integrating Structured and Semistructured Data. Storing auxiliary data for efficient maintenance and lineage tracing of complex views. Querying Semistructured Heterogeneous Information. Temporal View Self-Maintenance. Maintaining Temporal Views over Non-Temporal Information Sources for Data Warehousing. Efficient PCS Call Setup Protocols. The TSIMMIS Project: Integration of Heterogeneous Information Sources. Towards Special-Purpose Indexes and Statistics for Uncertain Data. SimRank: a measure of structural-context similarity. Mining the space of graph properties. Efficient and Flexible Location Management Techniques for Wireless Communication Systems. User Profile Replication for Faster Location Lookup in Mobile Environments. The TSIMMIS Approach to Mediation: Data Models and Languages. A Temporal-Logic Based Compositional Proof System for Real-Time Message Passing. Making Views Self-Maintainable for Data Warehousing. Declarative Support for Sensor Data Cleaning. Models and Issues in Data Stream Systems. Characterizing Memory Requirements for Queries over Continuous Data Streams. Constraint Checking with Partial Information. Operator placement for in-network stream query processing. Flexible Time Management in Data Stream Systems. Optimization of continuous queries with shared expensive filters. Completeness and Incompleteness of Trace-Based Network Proof Systems. Efficient and Complete Tests for Database Integrity Constraint Checking. Better Termination Analysis for Active Databases. Using Delta Relations to Optimize Condition Evaluation in Active Databases. Deductive and Active Databases: Two Paradigms or Ends of a Spectrum? STREAM: The Stanford Stream Data Manager. Adaptive Ordering of Pipelined Stream Filters. The STRIP Rule System For Efficiently Maintaining Derived Data. Behavior of Database Production Rules: Termination, Confluence, and Observable Determinism. StreaMon: An Adaptive Engine for Stream Query Processing. Change Detection in Hierarchically Structured Information. Rethinking the Conference Reviewing Process - Panel. WSQ/DSQ: A Practical Approach for Combined Querying of Databases and the Web. Local Verification of Global Integrity Constraints in Distributed Databases. Information Translation, Mediation, and Mosaic-Based Browsing in the TSIMMIS System. Data streams: fresh current or stagnant backwater? (panel). The WHIPS Prototype for Data Warehouse Creation and Maintenance. Starburst II: The Extender Strikes Back! Adaptive Filters for Continuous Queries over Distributed Data Streams. Best-effort cache synchronization with source cooperation. Adaptive Precision Setting for Cached Approximate Values. On-Line Warehouse View Maintenance. LORE: A Lightweight Object REpository for Semistructured Data. Set-Oriented Production Rules in Relational Database Systems. TIP: A Temporal Extension to Informix. View Maintenance in a Warehousing Environment. Computing the median with uncertainty. A System Prototype for Warehouse View Maintenance. Resource Sharing in Continuous Sliding-Window Aggregates. Trio: A System for Data, Uncertainty, and Lineage. Vision Paper: Enabling Privacy for the Paranoids. An Algebraic Approach to Rule Analysis in Expert Database Systems. Database Publication Practices. ULDBs: Databases with Uncertainty and Lineage. Content-Based Routing: Different Plans for Different Data. Practical Applications of Triggers and Constraints: Success and Lingering Issues (10-Year Award). Deriving Production Rules for Constraint Maintainance. Deriving Production Rules for Incremental View Maintenance. Production Rules in Parallel and Distributed Database Environments. Managing Semantic Heterogeneity with Production Rules and Persistent Queues. Lineage Tracing for General Data Warehouse Transformations. DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases. Performance Issues in Incremental Warehouse Maintenance. Query Optimization for XML. Offering a Precision-Performance Tradeoff for Aggregation Queries over Replicated Data. Query Optimization over Web Services. Memory-Limited Execution of Windowed Stream Joins. Implementing Set-Oriented Production Rules as an Extension to Starburst. A Unified Approach for Querying Structured Data and XML. Querying XML with Lore. From Semistructured Data to XML: Migrating the Lore Data Model and Query Language. Interactive Query and Search in Semistructured Databases. Scaling personalized web search. Schema Design for Uncertain Databases. Adaptive Caching for Continuous Queries. Representing and Querying Changes in Semistructured Data. A Toolkit for Constraint Management in Heterogeneous Information Systems. Practical Lineage Tracing in Data Warehouses. Lineage Tracing in a Data Warehousing System. A Pipelined Framework for Online Cleaning of Sensor Data Streams. Clustering Association Rules. Object Exchange Across Heterogeneous Information Sources. Working Models for Uncertain Data. Incremental Computation and Maintenance of Temporal Aggregates. Exploiting Lineage for Confidence Computation in Uncertain and Probabilistic Databases. Confidence-Aware Join Algorithms. The Pipelined Set Cover Problem. Indexing Relational Database Content Offline for Efficient Keyword-Based Search.'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorpaperlist = data_from_file_conv.map(lambda row: (row[0], [row[2]]))\n",
    "authorpapers = authorpaperlist.reduceByKey(lambda a,b: [\" \".join(a+b)])\n",
    "authorpapers.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def get_tokens(line):\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuations from each word\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatizing the words, see https://en.wikipedia.org/wiki/Lemmatisation\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    return (words)\n",
    "\n",
    "authortitles_rdd = authorpapers.map(lambda line: ( str(line[0]) , get_tokens(line[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Abraham Silberschatz',\n",
       "  ['transaction',\n",
       "   'management',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'data',\n",
       "   'model',\n",
       "   'storage',\n",
       "   'retrieval',\n",
       "   'continuous',\n",
       "   'medium',\n",
       "   'data',\n",
       "   'serializability',\n",
       "   'multilevel',\n",
       "   'monitor',\n",
       "   'environment',\n",
       "   'overview',\n",
       "   'multidatabase',\n",
       "   'transaction',\n",
       "   'management',\n",
       "   'storage',\n",
       "   'retrieval',\n",
       "   'continuous',\n",
       "   'medium',\n",
       "   'data',\n",
       "   'adaptive',\n",
       "   'commitment',\n",
       "   'distributed',\n",
       "   'realtime',\n",
       "   'transaction',\n",
       "   'performance',\n",
       "   'analysis',\n",
       "   'storage',\n",
       "   'system',\n",
       "   'efficient',\n",
       "   'global',\n",
       "   'transaction',\n",
       "   'management',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'efficiently',\n",
       "   'monitoring',\n",
       "   'bandwidth',\n",
       "   'latency',\n",
       "   'ip',\n",
       "   'network',\n",
       "   'topology',\n",
       "   'discovery',\n",
       "   'heterogeneous',\n",
       "   'ip',\n",
       "   'network',\n",
       "   'optimal',\n",
       "   'isp',\n",
       "   'subscription',\n",
       "   'internet',\n",
       "   'multihoming',\n",
       "   'algorithm',\n",
       "   'design',\n",
       "   'implication',\n",
       "   'analysis',\n",
       "   'biased',\n",
       "   'nontwophase',\n",
       "   'locking',\n",
       "   'protocol',\n",
       "   'subjective',\n",
       "   'measure',\n",
       "   'interestingness',\n",
       "   'knowledge',\n",
       "   'discovery',\n",
       "   'kernel',\n",
       "   'support',\n",
       "   'recoverablepersistent',\n",
       "   'virtual',\n",
       "   'memory',\n",
       "   'movetorear',\n",
       "   'list',\n",
       "   'scheduling',\n",
       "   'new',\n",
       "   'scheduling',\n",
       "   'algorithm',\n",
       "   'providing',\n",
       "   'qos',\n",
       "   'guarantee',\n",
       "   'systemwide',\n",
       "   'multiresolution',\n",
       "   'distributed',\n",
       "   'multilevel',\n",
       "   'recovery',\n",
       "   'mainmemory',\n",
       "   'database',\n",
       "   'nonserializable',\n",
       "   'execution',\n",
       "   'heterogeneous',\n",
       "   'distributed',\n",
       "   'database',\n",
       "   'system',\n",
       "   'theory',\n",
       "   'relaxed',\n",
       "   'atomicity',\n",
       "   'extended',\n",
       "   'abstract',\n",
       "   'multiversion',\n",
       "   'concurrency',\n",
       "   'scheme',\n",
       "   'rollback',\n",
       "   'concurrency',\n",
       "   'control',\n",
       "   'graph',\n",
       "   'protocol',\n",
       "   'using',\n",
       "   'edge',\n",
       "   'lock',\n",
       "   'efficient',\n",
       "   'acurate',\n",
       "   'cost',\n",
       "   'model',\n",
       "   'parallel',\n",
       "   'query',\n",
       "   'optimization',\n",
       "   'throughputcompetitive',\n",
       "   'admission',\n",
       "   'control',\n",
       "   'continuous',\n",
       "   'medium',\n",
       "   'database',\n",
       "   'view',\n",
       "   'maintenance',\n",
       "   'issue',\n",
       "   'chronicle',\n",
       "   'data',\n",
       "   'model',\n",
       "   'axiomatic',\n",
       "   'approach',\n",
       "   'deciding',\n",
       "   'query',\n",
       "   'safety',\n",
       "   'deductive',\n",
       "   'database',\n",
       "   'ensuring',\n",
       "   'transaction',\n",
       "   'atomicity',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'compatibility',\n",
       "   'commutativity',\n",
       "   'nontwophase',\n",
       "   'locking',\n",
       "   'protocol',\n",
       "   'multimedia',\n",
       "   'support',\n",
       "   'database',\n",
       "   'safety',\n",
       "   'recursive',\n",
       "   'horn',\n",
       "   'clause',\n",
       "   'infinite',\n",
       "   'relation',\n",
       "   'strict',\n",
       "   'history',\n",
       "   'objectbased',\n",
       "   'database',\n",
       "   'system',\n",
       "   'correctness',\n",
       "   'nonserializable',\n",
       "   'execution',\n",
       "   'datavalue',\n",
       "   'partitioning',\n",
       "   'virtual',\n",
       "   'message',\n",
       "   'annotation',\n",
       "   'distributed',\n",
       "   'programming',\n",
       "   'logic',\n",
       "   'periodic',\n",
       "   'resource',\n",
       "   'scheduling',\n",
       "   'continuous',\n",
       "   'medium',\n",
       "   'database',\n",
       "   'strong',\n",
       "   'recoverability',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'storage',\n",
       "   'retrieval',\n",
       "   'multimedia',\n",
       "   'object',\n",
       "   'relaxing',\n",
       "   'serializability',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'algorithm',\n",
       "   'provisioning',\n",
       "   'virtual',\n",
       "   'private',\n",
       "   'network',\n",
       "   'hose',\n",
       "   'model',\n",
       "   'provider',\n",
       "   'portal',\n",
       "   'application',\n",
       "   'modeling',\n",
       "   'performance',\n",
       "   'characteristic',\n",
       "   'serpentine',\n",
       "   'tape',\n",
       "   'drive',\n",
       "   'datablitz',\n",
       "   'storage',\n",
       "   'manager',\n",
       "   'main',\n",
       "   'memory',\n",
       "   'database',\n",
       "   'performance',\n",
       "   'critical',\n",
       "   'application',\n",
       "   'update',\n",
       "   'propagation',\n",
       "   'protocol',\n",
       "   'replicated',\n",
       "   'database',\n",
       "   'multidatabase',\n",
       "   'update',\n",
       "   'issue',\n",
       "   'reliable',\n",
       "   'transaction',\n",
       "   'management',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'bifocal',\n",
       "   'sampling',\n",
       "   'skewresistant',\n",
       "   'join',\n",
       "   'size',\n",
       "   'estimation',\n",
       "   'framework',\n",
       "   'parallel',\n",
       "   'processing',\n",
       "   'datalog',\n",
       "   'query',\n",
       "   'random',\n",
       "   'io',\n",
       "   'scheduling',\n",
       "   'online',\n",
       "   'tertiary',\n",
       "   'storage',\n",
       "   'system',\n",
       "   'deadlock',\n",
       "   'removal',\n",
       "   'using',\n",
       "   'partial',\n",
       "   'rollback',\n",
       "   'database',\n",
       "   'system',\n",
       "   'optimistic',\n",
       "   'commit',\n",
       "   'protocol',\n",
       "   'distributed',\n",
       "   'transaction',\n",
       "   'management',\n",
       "   'concurrency',\n",
       "   'control',\n",
       "   'problem',\n",
       "   'multidatabases',\n",
       "   'characteristic',\n",
       "   'solution',\n",
       "   'faulttolerant',\n",
       "   'architecture',\n",
       "   'continuous',\n",
       "   'medium',\n",
       "   'server',\n",
       "   'distributed',\n",
       "   'processing',\n",
       "   'logic',\n",
       "   'program',\n",
       "   'building',\n",
       "   'appliance',\n",
       "   'component',\n",
       "   'using',\n",
       "   'pebble',\n",
       "   'scalable',\n",
       "   'nonintrusive',\n",
       "   'load',\n",
       "   'sharing',\n",
       "   'ownerbased',\n",
       "   'distributed',\n",
       "   'system',\n",
       "   'retrofitting',\n",
       "   'quality',\n",
       "   'service',\n",
       "   'timesharing',\n",
       "   'operating',\n",
       "   'system',\n",
       "   'signaled',\n",
       "   'receiver',\n",
       "   'processing',\n",
       "   'pebble',\n",
       "   'componentbased',\n",
       "   'operating',\n",
       "   'system',\n",
       "   'lightweight',\n",
       "   'security',\n",
       "   'primitive',\n",
       "   'ecommerce',\n",
       "   'datablitz',\n",
       "   'high',\n",
       "   'performance',\n",
       "   'mainmemory',\n",
       "   'storage',\n",
       "   'manager',\n",
       "   'garbage',\n",
       "   'collection',\n",
       "   'object',\n",
       "   'oriented',\n",
       "   'database',\n",
       "   'using',\n",
       "   'transactional',\n",
       "   'cyclic',\n",
       "   'reference',\n",
       "   'counting',\n",
       "   'database',\n",
       "   'system',\n",
       "   'realtime',\n",
       "   'event',\n",
       "   'aggregation',\n",
       "   'telecommunication',\n",
       "   'information',\n",
       "   'communication',\n",
       "   'money',\n",
       "   'charge',\n",
       "   'meter',\n",
       "   'logical',\n",
       "   'physical',\n",
       "   'versioning',\n",
       "   'main',\n",
       "   'memory',\n",
       "   'database',\n",
       "   'obtaining',\n",
       "   'progressive',\n",
       "   'protocol',\n",
       "   'simple',\n",
       "   'multiversion',\n",
       "   'database',\n",
       "   'model',\n",
       "   'modeling',\n",
       "   'skewed',\n",
       "   'distribution',\n",
       "   'using',\n",
       "   'multifractals',\n",
       "   'law',\n",
       "   'resource',\n",
       "   'scheduling',\n",
       "   'enhanced',\n",
       "   'payperview',\n",
       "   'continuous',\n",
       "   'medium',\n",
       "   'database',\n",
       "   'theory',\n",
       "   'correct',\n",
       "   'locking',\n",
       "   'protocol',\n",
       "   'database',\n",
       "   'system',\n",
       "   'dal',\n",
       "   'high',\n",
       "   'performance',\n",
       "   'main',\n",
       "   'memory',\n",
       "   'storage',\n",
       "   'manager',\n",
       "   'recovering',\n",
       "   'mainmemory',\n",
       "   'lapse',\n",
       "   'efficient',\n",
       "   'deadlock',\n",
       "   'removal',\n",
       "   'scheme',\n",
       "   'nontwophase',\n",
       "   'locking',\n",
       "   'protocol',\n",
       "   'nontwophase',\n",
       "   'locking',\n",
       "   'protocol',\n",
       "   'shared',\n",
       "   'exclusive',\n",
       "   'lock',\n",
       "   'formal',\n",
       "   'approach',\n",
       "   'recovery',\n",
       "   'compensating',\n",
       "   'transaction',\n",
       "   'triggered',\n",
       "   'realtime',\n",
       "   'database',\n",
       "   'consistency',\n",
       "   'constraint',\n",
       "   'challenge',\n",
       "   'global',\n",
       "   'information',\n",
       "   'system',\n",
       "   'lowcost',\n",
       "   'storage',\n",
       "   'server',\n",
       "   'movie',\n",
       "   'demand',\n",
       "   'database',\n",
       "   'scientific',\n",
       "   'journal',\n",
       "   'extinction',\n",
       "   'explosion',\n",
       "   'panel',\n",
       "   'mr',\n",
       "   'diagram',\n",
       "   'model',\n",
       "   'conceptual',\n",
       "   'database',\n",
       "   'design',\n",
       "   'discovery',\n",
       "   'interesting',\n",
       "   'pattern',\n",
       "   'association',\n",
       "   'rule',\n",
       "   'multiresolution',\n",
       "   'relational',\n",
       "   'data',\n",
       "   'model',\n",
       "   'panace',\n",
       "   'system',\n",
       "   'us',\n",
       "   'database',\n",
       "   'technology',\n",
       "   'manage',\n",
       "   'network',\n",
       "   'safety',\n",
       "   'recursive',\n",
       "   'horn',\n",
       "   'clause',\n",
       "   'function',\n",
       "   'symbol',\n",
       "   'controlling',\n",
       "   'concurrency',\n",
       "   'using',\n",
       "   'locking',\n",
       "   'protocol',\n",
       "   'preliminary',\n",
       "   'report',\n",
       "   'eliminating',\n",
       "   'cascading',\n",
       "   'rollback',\n",
       "   'structured',\n",
       "   'database',\n",
       "   'storage',\n",
       "   'retrieval',\n",
       "   'continous',\n",
       "   'medium',\n",
       "   'data',\n",
       "   'abstract',\n",
       "   'design',\n",
       "   'evaluation',\n",
       "   'redistribution',\n",
       "   'strategy',\n",
       "   'widearea',\n",
       "   'commodity',\n",
       "   'distribution',\n",
       "   'failure',\n",
       "   'tolerant',\n",
       "   'centralized',\n",
       "   'mutual',\n",
       "   'exclusion',\n",
       "   'algorithm',\n",
       "   'transaction',\n",
       "   'model',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'exploiting',\n",
       "   'transaction',\n",
       "   'semantics',\n",
       "   'multidatabase',\n",
       "   'system',\n",
       "   'using',\n",
       "   'codewords',\n",
       "   'protect',\n",
       "   'database',\n",
       "   'data',\n",
       "   'class',\n",
       "   'software',\n",
       "   'error',\n",
       "   'new',\n",
       "   'forgotten',\n",
       "   'dream',\n",
       "   'database',\n",
       "   'research',\n",
       "   'panel',\n",
       "   'analysis',\n",
       "   'technique',\n",
       "   'transitive',\n",
       "   'closure',\n",
       "   'algorithm',\n",
       "   'statistical',\n",
       "   'approach',\n",
       "   'scheduling',\n",
       "   'data',\n",
       "   'replication',\n",
       "   'improve',\n",
       "   'tape',\n",
       "   'jukebox',\n",
       "   'performance',\n",
       "   'unilateral',\n",
       "   'commit',\n",
       "   'new',\n",
       "   'paradigm',\n",
       "   'reliable',\n",
       "   'distributed',\n",
       "   'transaction',\n",
       "   'processing',\n",
       "   'periodic',\n",
       "   'retrieval',\n",
       "   'video',\n",
       "   'disk',\n",
       "   'array',\n",
       "   'cyclic',\n",
       "   'association',\n",
       "   'rule',\n",
       "   'disk',\n",
       "   'scheduling',\n",
       "   'quality',\n",
       "   'service',\n",
       "   'guarantee',\n",
       "   'framework',\n",
       "   'storage',\n",
       "   'retrieval',\n",
       "   'continous',\n",
       "   'medium',\n",
       "   'data',\n",
       "   'buffer',\n",
       "   'replacement',\n",
       "   'algorithm',\n",
       "   'multimedia',\n",
       "   'storage',\n",
       "   'system',\n",
       "   'disk',\n",
       "   'striping',\n",
       "   'video',\n",
       "   'server',\n",
       "   'environment',\n",
       "   'demand',\n",
       "   'paging',\n",
       "   'videoondemand',\n",
       "   'server',\n",
       "   'mapping',\n",
       "   'homogeneous',\n",
       "   'graph',\n",
       "   'linear',\n",
       "   'arrayprocessor',\n",
       "   'model',\n",
       "   'extending',\n",
       "   'concurrent',\n",
       "   'pascal',\n",
       "   'allow',\n",
       "   'dynamic',\n",
       "   'resource',\n",
       "   'management',\n",
       "   'abstract'])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((authortitles_rdd.count()))\n",
    "authortitles_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(author='JiHoon Park', words=['duallan', 'topology', 'dualpath', 'ethernet', 'module', 'research', 'note', 'xmlbased', 'approach', 'software', 'process', 'improvement', 'internet']),\n",
       " Row(author='Gilles Parmentier', words=['cachebased', 'parallelization', 'multiple', 'sequence', 'alignment', 'problem', 'construction', 'phylogenetic', 'tree', 'parallel', 'cluster', 'bgee', 'integrating', 'comparing', 'heterogeneous', 'transcriptome', 'data', 'among', 'specie']),\n",
       " Row(author='Jrg Fischer', words=['real', 'pram', 'programming', 'integrated', 'object', 'model', 'activity', 'network', 'based', 'simulation', 'formalizing', 'timing', 'diagram', 'causal', 'dependency', 'verification', 'purpose', 'clinical', 'use', 'multimodal', 'resource', 'robot', 'assisted', 'functional', 'neurosurgery', 'development', 'platform', 'design', 'optimization', 'mobile', 'radio', 'network', 'scaled', 'cgem', 'fast', 'accelerated', 'em'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authortitles_df = spark.createDataFrame(authortitles_rdd, [\"author\", \"words\"])\n",
    "authortitles_df.cache()\n",
    "authortitles_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              author|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|         JiHoon Park|[duallan, topolog...|(163258,[10,24,39...|\n",
      "|   Gilles Parmentier|[cachebased, para...|(163258,[6,28,36,...|\n",
      "|         Jrg Fischer|[real, pram, prog...|(163258,[2,3,4,9,...|\n",
      "|     Cdric Lichtenau|[real, pram, prog...|(163258,[0,38,64,...|\n",
      "|  Gerald G. Pechanek|[manarray, proces...|(163258,[0,2,11,1...|\n",
      "|Jan Bkgaard Pedersen|[pvmbuilder, tool...|(163258,[0,4,25,2...|\n",
      "|      Alan S. Wagner|[pvmbuilder, tool...|(163258,[0,2,8,20...|\n",
      "|      Marco Pedicini|[scheduling, v, c...|(163258,[28,45,52...|\n",
      "|        Andr Schiper|[exploiting, atom...|(163258,[0,1,2,3,...|\n",
      "|   Joo Gabriel Silva|[wmpi, library, e...|(163258,[0,1,3,4,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2)\n",
    "\n",
    "cv_model = cv.fit(authortitles_df)\n",
    "\n",
    "authtitles_df_w_features = cv_model.transform(authortitles_df)\n",
    "authtitles_df_w_features.cache()\n",
    "authtitles_df_w_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda = LDA(k=10, maxIter=5)\n",
    "\n",
    "lda_model = lda.fit(authtitles_df_w_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+---------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices          |termWeights                                                                                                       |\n",
      "+-----+---------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[1, 11, 3, 4, 0]     |[0.01719493825020843, 0.01235987204557846, 0.009442120800160367, 0.007818244909483298, 0.006997585511421438]      |\n",
      "|1    |[0, 249, 225, 234, 5]|[0.0027651047359932603, 0.0025642233725602513, 0.002397230096790655, 0.0018621328120444253, 0.0017399240715086015]|\n",
      "|2    |[6, 2, 3, 0, 5]      |[0.00157914067946371, 0.0012263706378067918, 0.0011641039796168378, 0.001116592655212018, 0.0010074761539517628]  |\n",
      "|3    |[0, 9, 4, 24, 1]     |[0.019314338454217473, 0.008234841858406398, 0.0077065679488328405, 0.007262733499546979, 0.007044103693409012]   |\n",
      "|4    |[2, 5, 1, 13, 16]    |[0.0021110477184515565, 0.0015313706727758693, 0.0015303742643079936, 0.001422371360979004, 0.001384010488330905] |\n",
      "|5    |[1, 0, 6, 4, 13]     |[0.002422458802072094, 0.00240908782815323, 0.0015350823610831544, 0.0014273910127609111, 0.0011804195132708883]  |\n",
      "|6    |[1, 0, 7, 4, 18]     |[0.0032688471731734756, 0.003128768839100365, 0.0014002331283100443, 0.001363152508233143, 0.0013041160772390924] |\n",
      "|7    |[0, 2, 1, 33, 6]     |[0.007334218816944729, 0.002642056930316159, 0.0023728340020895667, 0.002157950575345068, 0.0019405783740362873]  |\n",
      "|8    |[2, 0, 5, 1, 3]      |[0.016859158573858782, 0.015133843570691703, 0.009942279007210545, 0.00966629549170759, 0.008973811615456087]     |\n",
      "|9    |[0, 3, 4, 1, 76]     |[0.0013245891297059044, 9.887323182987758E-4, 9.056011414879747E-4, 8.782817639822479E-4, 7.648886845037857E-4]   |\n",
      "+-----+---------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "['using' 'image' 'based' 'model' 'system']\n",
      "['system' 'der' 'und' 'von' 'algorithm']\n",
      "['data' 'network' 'based' 'system' 'algorithm']\n",
      "['system' 'design' 'model' 'software' 'using']\n",
      "['network' 'algorithm' 'using' 'service' 'web']\n",
      "['using' 'system' 'data' 'model' 'service']\n",
      "['using' 'system' 'analysis' 'model' 'control']\n",
      "['system' 'network' 'using' 'simulation' 'data']\n",
      "['network' 'system' 'algorithm' 'using' 'based']\n",
      "['system' 'based' 'model' 'using' 'grid']\n"
     ]
    }
   ],
   "source": [
    "# Describe topics\n",
    "topics = lda_model.describeTopics(5)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the results\n",
    "import numpy as np\n",
    "topic_i = topics.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(cv_model.vocabulary)[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.4**  The words are very similar to A4.  Mostly technical words and system and network are some of the most common.  Using is also up there.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.5 (max points 5)** In addition, calculate the topic density vector for each author and use the topic density to calculate the cosine simularity for each pair of authors. For example, if the topic density for author X is $[x_1, x_2, x_3, \\dots]$ and topic density vector for author Y is $[y_1, y_2, y_3, \\dots]$, then the cosine simularity is $\\frac{x_1\\cdot y_1 + x_2\\cdot y_2 + x_3\\cdot y_3 +\\dots}{\\sqrt{x_1^2+ x_2^2+ x_3^2 +\\dots}\\sqrt{y_1^2+ y_2^2+ y_3^2 +\\dots}}$. Show the 10 most similar author pairs and comment on their similarity, if possible taking into consideration the results from the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not able to finish this one but here is the start of my code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(163258,[10,24,39...|\n",
      "|(163258,[6,28,36,...|\n",
      "|(163258,[2,3,4,9,...|\n",
      "|(163258,[0,38,64,...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authtitles_df_w_features.select(\"features\").show(4)\n",
    "\n",
    "\n",
    "\n",
    "def densecalc(data):\n",
    "    density = np.zeros(data[0])\n",
    "    sumwords = sum(data[2])\n",
    "    for i in range(len(data[1])):\n",
    "        wordind = data[1][i]\n",
    "        desity[int(wordind)] = data[2][i]/sumwords\n",
    "    return denscalc\n",
    "\n",
    "\n",
    "def cossim(a,b):\n",
    "    numerator = sum(a*b)\n",
    "    denominator = sqrt(sum(a**2))*sqrt(sum(b**2))\n",
    "    return (numerator/denominator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = authtitles_df_w_features.select(\"features\").rdd.map(lambda r: densecalc(r[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_df = spark.createDataFrame(densities, [\"densities\"])\n",
    "dense_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
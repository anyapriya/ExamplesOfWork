{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "## Homework 1: Spark RDDs, Spark SQL and Hive\n",
    "### Milan Vojnovic and Christine Yuen, edited by Simon Schoeller LT 2019\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "**Deadline**: February 27, 2019, 5pm London time\n",
    "\n",
    "**Datasets**: All the required datasets are available for download from here:\n",
    "\n",
    "https://www.dropbox.com/sh/89xbpcjl4oq0j4w/AACrbtUzm3oCW1OcpL7BasRfa?dl=0\n",
    "in the respective sub-directories.\n",
    "\n",
    "Please make sure that you document your work appropriately. If you get stuck somewhere, make sure to give the other parts a try.\n",
    "\n",
    "\n",
    "## A. Spark RDDs (30 points)\n",
    "\n",
    "We continue to analyse the dblp dataset available in the file `author-large.txt`. This time, we want to find the top 10 pairs of authors who published the largest number of papers together (with possible other collaborators). For example, if authors $a$, $b$ and $c$ published a paper with title $t$, then this contributes one joint publication for each author pair ($a$,$b$), ($b$,$c$) and ($a$,$c$). Use the first column of the input data for the author names and use the third column of the input data for the publication title. \n",
    "\n",
    "You need to solve this task by using RDD operations in pyspark like those in `pyspark_rdd.ipynb` in week 3 of the course and the [Spark RDD documentation]( http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD). You can run your code on your laptop or GCP. Please make sure to give us all your code and document what you have done outside of the notebook, for example using terminal in- and output or screenshots. *Please make sure to delete you dataproc clusters and buckets afterwards.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "First, here is the code to create the bucket, the cluster, and start up the jupyter notebook on the cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>gsutil mb gs://anyabucket24feb2019/\n",
    "\n",
    "gsutil cp \"/Users/Anya/Documents/Grad School/LSE/Dist Comp for Big Data/author-large.txt\" gs://anyabucket24feb2019/\n",
    "\n",
    "gcloud dataproc clusters create anyacluster24feb2019 --project my-project-1519696393583 --bucket anyabucket24feb2019 --initialization-actions gs://dataproc-initialization-actions/jupyter/jupyter.sh\n",
    "\n",
    "22\n",
    "\n",
    "\n",
    "export PORT=8123\n",
    "export HOSTNAME=\"anyacluster24feb2019-m\"\n",
    "export PROJECT=\"my-project-1519696393583\"\n",
    "export ZONE=\"europe-west1-b\"\n",
    "\n",
    "\n",
    "gcloud compute ssh ${HOSTNAME} \\\n",
    "    --project=${PROJECT} --zone=${ZONE}  -- \\\n",
    "    -D ${PORT} -N &\n",
    "\n",
    "\"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\" \\\n",
    "      --proxy-server=\"socks5://localhost:${PORT}\" \\\n",
    "      --user-data-dir=/tmp/${HOSTNAME}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, here is my code to use pyspark to obtain the top 10 pairs of authors who published the most works together.  First we import the libraries, then load in the data.  After this, we convert the data, and then obtain the data of just the papers and author columns.  Then we join this data with itself by the first column (the title of the papers), such that will obtain data of two rows: one of tuples of the pairs of authors (some duplicates and all unsorted) and the other with the paper titles.  Then we create a function sortauthors that marks duplicates (a pair of the same author) and otherwise sorts the names alphabetical such that we will not double count the pair of (a,b) and (b,a), and can group them together.  This function also changes the pairs of authors to be the first entry and title of the paper to be the second, so that the author pairs are now the key.  Then we use the function on each row, and only take the distinct combinations of author pairs and paper titles.  Then we filter out any that were marked as pairs of the same person.  Then we count the number of occurrences of each pair, which we sort and then print out the top 10.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Irith Pomeranz', 'Sudhakar M. Reddy'), 246),\n",
       " (('Amr El Abbadi', 'Divyakant Agrawal'), 161),\n",
       " (('Makoto Takizawa', 'Tomoya Enokido'), 137),\n",
       " (('Didier Dubois', 'Henri Prade'), 122),\n",
       " (('Elizabeth Chang', 'Tharam S. Dillon'), 115),\n",
       " (('Mary Jane Irwin', 'Narayanan Vijaykrishnan'), 107),\n",
       " (('Mahmut T. Kandemir', 'Mary Jane Irwin'), 100),\n",
       " (('Chun Chen', 'Jiajun Bu'), 99),\n",
       " (('Shojiro Nishio', 'Takahiro Hara'), 96),\n",
       " (('Filip De Turck', 'Piet Demeester'), 90)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data_from_file = sc.\\\n",
    "    textFile(\n",
    "        \"gs://anyabucket24feb2019/author-large.txt\", \n",
    "        4)\n",
    "\n",
    "data_from_file_conv = data_from_file.map(lambda row: np.array(row.strip().split(\"\\t\")))\n",
    "paper_author = data_from_file_conv.map(lambda row: (row[2], row[0]))\n",
    "pairedAuths = paper_author.join(paper_author)\n",
    "\n",
    "def sortauthors(row):\n",
    "    if row[1][0] == row[1][1]:\n",
    "        return \"SamePerson\"\n",
    "    else :\n",
    "        return (tuple(sorted(row[1])), row[0])\n",
    "\n",
    "updatedpairs = pairedAuths.map(sortauthors).distinct()\n",
    "updatedpairs = updatedpairs.filter(lambda row: row != \"SamePerson\")\n",
    "\n",
    "paircounts = updatedpairs.countByKey()\n",
    "sortedcounts = [(i, paircounts[i]) for i in sorted(paircounts, key=paircounts.get, reverse=True)]\n",
    "sortedcounts[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Spark SQL (30 points)\n",
    "\n",
    "Do the same as in problem A, but this time use the Spark SQL API, which we covered in week 4. You may find useful to consult 'Querying with Spark SQL' in `spark-dataframe-sql.ipynb` of week 4 class and the [Spark SQL documentation](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Next we will do the same using spark sql.  First we specify the schema of the data.  Then we load it in.  After this, we run a query just to show that there are duplicates of the same author pair and same title.  Thus we need to be sure to only take distinct combinations of authors and titles in our final query.  In our actual query, we join the distinct combinations of authors and titles with itself using the titles, then we filter this where the first author comes before the second (eliminating pairs of the same author and duplicates of [a,b] vs [b,a]) and then we group these by the pairs of authors, and then order it by descending count, and then show the count and both author names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|count(1)|               title|\n",
      "+--------+--------------------+\n",
      "|       4|A Low Power Pseud...|\n",
      "+--------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+------------------+--------------------+\n",
      "|count(1)|            author|              author|\n",
      "+--------+------------------+--------------------+\n",
      "|     246|    Irith Pomeranz|   Sudhakar M. Reddy|\n",
      "|     161|     Amr El Abbadi|   Divyakant Agrawal|\n",
      "|     137|   Makoto Takizawa|      Tomoya Enokido|\n",
      "|     122|     Didier Dubois|         Henri Prade|\n",
      "|     115|   Elizabeth Chang|    Tharam S. Dillon|\n",
      "|     107|   Mary Jane Irwin|Narayanan Vijaykr...|\n",
      "|     100|Mahmut T. Kandemir|     Mary Jane Irwin|\n",
      "|      99|         Chun Chen|           Jiajun Bu|\n",
      "|      96|    Shojiro Nishio|       Takahiro Hara|\n",
      "|      90|    Filip De Turck|      Piet Demeester|\n",
      "+--------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"author\", StringType(), True),    \n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"year\", LongType(), True)\n",
    "])\n",
    "\n",
    "authors = spark.read.csv(\"gs://anyabucket24feb2019/author-large.txt\", \n",
    "                    header='false', schema=schema, sep='\\t')\n",
    "authors.createOrReplaceTempView(\"authors\")\n",
    "\n",
    "\n",
    "spark.sql(\"select count(*), a.title from authors a join authors b on a.title = b.title where \\\n",
    "    a.author = 'Irith Pomeranz' and b.author = 'Sudhakar M. Reddy' group by a.title order by count(*) desc\").show(1)\n",
    "\n",
    "\n",
    "spark.sql(\"select count(*), a.author, b.author from(select distinct author, title from authors)a \\\n",
    "                                               join(select distinct author, title from authors)b on a.title = b.title \\\n",
    "          where a.author < b.author group by a.author, b.author order by count(*) desc\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Hive (40 points)\n",
    "\n",
    "In this part we are going to use the Yelp data available in the following JSON file `Yelp/yelp_academic_dataset_user.json`. You may complete this task by using either Hive installed on your laptop or using Hive on Google Cloud Platform. Please complete the following steps:\n",
    "\n",
    "_(Here, it is particularly important that you find a suitable way to document your work appropriately.)_\n",
    "\n",
    "### 1. Load data into a Hive table\n",
    "\n",
    "Create a Hive table and load the input data into this table.\n",
    "\n",
    "Please describe any commmands that you run in a command line interface, provide all the code that you wrote and ran. For example, this may include any commands run in a terminal, Hive script files (\\*.sql), and screenshots (if, for example, you used Google Cloud Platform through the browser interface). See the class examples for references.\n",
    "\n",
    "Note:\n",
    "* The dataset is in JSON format whereas in the class the datasets were in XML or TXT format. You will need to figure out (look up) how to load data from a JSON file to a Hive table. \n",
    "* You will need to infer the schema by looking at the data. \n",
    "\n",
    "Hints: \n",
    "\n",
    "* Some of the columns are of array type. For example, you should use array&lt;STRING&gt; for the friends column.\n",
    "* The size of the dataset is large (about 1GB). You may want to create a smaller dataset first and work with this smaller dataset until you develop and test your code, and then apply it on the original dataset.\n",
    "\n",
    "\n",
    "### 2. Simple queries\n",
    "\n",
    "Having created the Hive table and loaded the data into it, write and execute queries to:\n",
    "\n",
    "i. retrieve the schema;\n",
    "\n",
    "ii. show the number of rows in the table;\n",
    "\n",
    "iii. select top 10 users who have provided the largest number of reviews (the output should consist of the user name and the number of reviews of the users).\n",
    "\n",
    "For all the queries, please show both the commands you used and the output. You may copy and paste the commands that you run and the outputs, or provide screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "\n",
    "The code used is shown below.  First I created an ssh connection with the cluster created above.  Then I downloaded the file to the cluster from dropbox.  Next I loaded it into hadoop.  Then I started hive, added the jar, created the database, defined the schema, and made sure to specify the serde and the location of the file within hadoop.  The last three lines are the queries for the actual assignment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "gcloud compute --project \"my-project-1519696393583\" ssh --zone \"europe-west1-b\" \"anyacluster24feb2019-m\"\n",
    "\n",
    "wget \"https://www.dropbox.com/sh/89xbpcjl4oq0j4w/AAC4_qW_wKyGIXXYZOwZC-Wia/Yelp/yelp_academic_dataset_user.json?dl=0\" -O yelpuserdata.json\n",
    "\n",
    "hadoop fs -put -f /home/Anya/yelpuserdata.json hdfs://anyacluster24feb2019-m/user/Anya/yelpuserdata.json\n",
    "\n",
    "hive\n",
    "\n",
    "ADD JAR /usr/lib/hive/lib/hive-hcatalog-core.jar;\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS yelp;\n",
    "\n",
    "USE yelp;\n",
    "\n",
    "CREATE EXTERNAL TABLE users (\n",
    "user_id STRING,\n",
    "name STRING,\n",
    "review_count INT,\n",
    "yelping_since STRING,\n",
    "friends array<STRING>,\n",
    "useful INT,\n",
    "funny INT,\n",
    "cool INT,\n",
    "fans INT, \n",
    "elite array<STRING>,\n",
    "average_stars DOUBLE,\n",
    "compliment_hot INT,\n",
    "compliment_more INT,\n",
    "compliment_profile INT,\n",
    "compliment_cute INT,\n",
    "compliment_list INT,\n",
    "compliment_note INT,\n",
    "compliment_plain INT,\n",
    "compliment_cool INT,\n",
    "compliment_funny INT,\n",
    "compliment_writer INT,\n",
    "compliment_photos INT,\n",
    "type STRING\n",
    ")\n",
    "ROW FORMAT SERDE 'org.openx.jarfiles.jsonserde.JsonSerDe' \n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs://anyacluster24feb2019-m/user/Anya/';\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DESCRIBE users;\n",
    "SELECT Count(*) FROM users;\n",
    "SELECT user_id, review_count FROM users ORDER BY review_count DESC limit 10;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. The code to show the schema and the result are shown below.  \n",
    "![2.i](Assignment1_C2i.png)\n",
    "\n",
    "ii. The query to obtain the number of rows in the table is shown below, as well as the result.  There are 1029432 rows in total.  \n",
    "![2.ii](Assignment1_C2ii.png)\n",
    "\n",
    "iii. The query to obtain top 10 users who have provided the largest number of reviews as well as the counts of their reviews is shown below, as well as the result.  \n",
    "![2.iii](Assignment1_C2iii.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
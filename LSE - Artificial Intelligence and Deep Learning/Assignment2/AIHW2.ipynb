{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIHW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "SjIkS3u3E_0i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eZoREML5JmFB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Please note that for all the following problems, 0 will be high effort and 1 will be low effort.*\n",
        "\n",
        "# Problem 1\n",
        "\n",
        "## 1.1\n",
        "\n",
        "Below we numerically compute the value function of each deterministic policy, and then these are ranked based on the value at the initial state.  The optimal policy is thus 1,1,1,0,0 or low, low, low, high, high.  Even in some of the other policies that are ranked highly, it is mostly playing low effort when closest to -3 (your opponent is closer to winning) and high effort when closest to 3 (you are closer to winning).  This is probably because when you are closer to -3, there is more chance of losing even if you put in the high effort, so the cost of high effort ends up being larger then the expected value from an unlikely win if you spend a high amount of energy.  Therefore, spend low energy and cut costs.  However, when closer to 3, you are more likely to win, and thus it becomes more valuable to put in high effort to increase the chances of the win even more.  "
      ]
    },
    {
      "metadata": {
        "id": "6TgyR19Zhlev",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "actions = [0, 1]\n",
        "policies = [p for p in itertools.product(actions, repeat=5)]\n",
        "\n",
        "d = 3\n",
        "states = np.arange(-d,d + 1)\n",
        "probs = [.55,.45]\n",
        "costs = [-50, -10]\n",
        "R = 1000\n",
        "\n",
        "# initialise state values\n",
        "state_values = np.zeros(len(states))\n",
        "state_values[d*2] = 1000\n",
        "theta = 1e-6\n",
        "\n",
        "allvalues = []\n",
        "initstatevalues = []\n",
        "\n",
        "for pol in policies:\n",
        "    while True:\n",
        "        delta = 0.0\n",
        "\n",
        "        for s in range(1,len(state_values)-1):\n",
        "            new_value = probs[pol[s-1]]*state_values[s+1] + (1-probs[pol[s-1]])*state_values[s-1] + costs[pol[s-1]]\n",
        "            delta = np.maximum(delta, np.abs(state_values[s] - new_value))\n",
        "            state_values[s] = new_value\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    allvalues.append(np.copy(state_values)) \n",
        "    initstatevalues.append(np.copy(state_values[3]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "otNdgCSTTif_",
        "colab_type": "code",
        "outputId": "d61e360f-9dbf-4467-a397-2c81e07ca4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        }
      },
      "cell_type": "code",
      "source": [
        "rankedorder = np.flipud(np.argsort(initstatevalues))\n",
        "for i in range(len(rankedorder)):\n",
        "    print(str(i + 1) + \". Initial State Value: \" + str(initstatevalues[rankedorder[i]]))\n",
        "    print(\"    Policy: \" + str(policies[rankedorder[i]] )  )\n",
        "    print(\"    All Values: \" + str(allvalues[rankedorder[i]]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Initial State Value: 281.6528907652884\n",
            "    Policy: (1, 1, 1, 0, 0)\n",
            "    All Values: [   0.           56.52453139  147.83229276  281.65289077  467.43362221\n",
            "  710.34512999 1000.        ]\n",
            "2. Initial State Value: 277.1650427015227\n",
            "    Policy: (1, 1, 1, 1, 0)\n",
            "    All Values: [   0.           55.31683793  145.14852965  277.1650427   460.74078161\n",
            "  707.33335173 1000.        ]\n",
            "3. Initial State Value: 269.2991861176355\n",
            "    Policy: (1, 1, 1, 0, 1)\n",
            "    All Values: [   0.           53.20011395  140.44469682  269.29918612  449.0102279\n",
            "  686.95562535 1000.        ]\n",
            "4. Initial State Value: 266.2135950190735\n",
            "    Policy: (1, 1, 1, 1, 1)\n",
            "    All Values: [   0.           52.3697722   138.5994929   266.21359502  444.40860792\n",
            "  684.42473435 1000.        ]\n",
            "5. Initial State Value: 261.6528905420873\n",
            "    Policy: (1, 1, 0, 0, 0)\n",
            "    All Values: [   0.           51.14247145  135.87215965  261.65289054  455.47348916\n",
            "  704.96307012 1000.        ]\n",
            "6. Initial State Value: 253.15737810536825\n",
            "    Policy: (1, 1, 0, 1, 0)\n",
            "    All Values: [   0.           48.85630374  130.79178684  253.15737811  444.18377158\n",
            "  699.88269721 1000.        ]\n",
            "7. Initial State Value: 252.57148040811245\n",
            "    Policy: (0, 1, 1, 0, 0)\n",
            "    All Values: [   0.           10.05224466  109.18590034  252.57148041  450.04274556\n",
            "  702.5192355  1000.        ]\n",
            "8. Initial State Value: 247.35092638582262\n",
            "    Policy: (1, 0, 1, 0, 0)\n",
            "    All Values: [   0.           36.01172874  102.24828698  247.35092639  446.92081958\n",
            "  701.11436881 1000.        ]\n",
            "9. Initial State Value: 244.98527834264854\n",
            "    Policy: (1, 1, 0, 0, 1)\n",
            "    All Values: [   0.           46.65716866  125.90481841  244.98527834  433.32383579\n",
            "  678.32810968 1000.        ]\n",
            "10. Initial State Value: 244.15382769140103\n",
            "    Policy: (0, 1, 1, 1, 0)\n",
            "    All Values: [   0.            7.06533559  103.7551566   244.15382769  437.97442643\n",
            "  697.0884919  1000.        ]\n",
            "11. Initial State Value: 237.5757600903662\n",
            "    Policy: (1, 1, 0, 1, 1)\n",
            "    All Values: [   0.           44.66324507  121.4738772   237.57576009  423.47729996\n",
            "  672.91251498 1000.        ]\n",
            "12. Initial State Value: 237.5757599958123\n",
            "    Policy: (0, 1, 1, 0, 1)\n",
            "    All Values: [   0.            4.73118456   99.51124387  237.57576     428.54350133\n",
            "  675.69892573 1000.        ]\n",
            "13. Initial State Value: 237.57575513166793\n",
            "    Policy: (1, 0, 1, 1, 0)\n",
            "    All Values: [   0.           32.97805496   95.50678961  237.57575513  433.4378247\n",
            "  695.04702112 1000.        ]\n",
            "14. Initial State Value: 237.03911973810176\n",
            "    Policy: (0, 1, 0, 0, 0)\n",
            "    All Values: [   0.            4.54076195   99.16502259  237.03911974  440.75429069\n",
            "  698.33943081 1000.        ]\n",
            "15. Initial State Value: 232.01420153202787\n",
            "    Policy: (1, 0, 0, 0, 0)\n",
            "    All Values: [   0.           31.25205553   91.67123538  232.01420153  437.74935621\n",
            "  696.9872103  1000.        ]\n",
            "16. Initial State Value: 230.90909365402132\n",
            "    Policy: (1, 0, 1, 0, 1)\n",
            "    All Values: [   0.           30.90909255   90.90909365  230.90909365  424.24242631\n",
            "  673.33333447 1000.        ]\n",
            "17. Initial State Value: 230.36711024795545\n",
            "    Policy: (0, 1, 1, 1, 1)\n",
            "    All Values: [   0.            2.17327668   94.86050219  230.36711025  418.20851936\n",
            "  670.01468565 1000.        ]\n",
            "18. Initial State Value: 224.24242167658974\n",
            "    Policy: (0, 1, 0, 1, 0)\n",
            "    All Values: [ 0.00000000e+00 -1.87536079e-06  9.09090883e+01  2.24242422e+02\n",
            "  4.24242422e+02  6.90909090e+02  1.00000000e+03]\n",
            "19. Initial State Value: 222.21437304589443\n",
            "    Policy: (1, 0, 1, 1, 1)\n",
            "    All Values: [   0.           28.21073099   84.91273464  222.21437305  412.24970812\n",
            "  666.73733947 1000.        ]\n",
            "20. Initial State Value: 217.57575993940202\n",
            "    Policy: (0, 1, 0, 0, 1)\n",
            "    All Values: [   0.           -2.36558967   86.608018    217.57575994  415.64027548\n",
            "  668.60215152 1000.        ]\n",
            "21. Initial State Value: 217.57575539235302\n",
            "    Policy: (0, 0, 1, 0, 0)\n",
            "    All Values: [   0.          -15.5260261    62.67995333  217.57575539  429.11506951\n",
            "  693.10178128 1000.        ]\n",
            "22. Initial State Value: 217.57575483464598\n",
            "    Policy: (1, 0, 0, 1, 0)\n",
            "    All Values: [   0.           26.77115824   81.71368587  217.57575483  419.64472103\n",
            "  688.84012446 1000.        ]\n",
            "23. Initial State Value: 210.99768710889873\n",
            "    Policy: (1, 0, 0, 0, 1)\n",
            "    All Values: [   0.           24.72969048   77.1770891   210.99768711  411.39635756\n",
            "  666.26799666 1000.        ]\n",
            "24. Initial State Value: 207.76698834554003\n",
            "    Policy: (0, 0, 0, 0, 0)\n",
            "    All Values: [   0.          -19.46908524   55.51075486  207.76698835  423.24936165\n",
            "  690.46221274 1000.        ]\n",
            "25. Initial State Value: 205.45454820478156\n",
            "    Policy: (0, 1, 0, 1, 1)\n",
            "    All Values: [   0.           -6.66666481   78.7878814   205.4545482   400.00000222\n",
            "  660.00000122 1000.        ]\n",
            "26. Initial State Value: 202.76093993306523\n",
            "    Policy: (0, 0, 1, 1, 0)\n",
            "    All Values: [   0.          -21.48148355   51.85184892  202.76093993  409.42760733\n",
            "  684.2424233  1000.        ]\n",
            "27. Initial State Value: 197.86834154257033\n",
            "    Policy: (0, 0, 1, 0, 1)\n",
            "    All Values: [   0.          -23.44827366   48.27586517  197.86834154  402.92581202\n",
            "  661.60919661 1000.        ]\n",
            "28. Initial State Value: 196.95449400280205\n",
            "    Policy: (1, 0, 0, 1, 1)\n",
            "    All Values: [   0.           20.37145808   67.49212828  196.954494    393.78733803\n",
            "  656.58303591 1000.        ]\n",
            "29. Initial State Value: 188.22143406436751\n",
            "    Policy: (0, 0, 0, 1, 0)\n",
            "    All Values: [   0.          -27.32626837   41.2249675   188.22143406  399.40036189\n",
            "  679.73016285 1000.        ]\n",
            "30. Initial State Value: 183.70370686218044\n",
            "    Policy: (0, 0, 1, 1, 1)\n",
            "    All Values: [   0.          -29.14236278   37.92297597  183.70370686  384.10237732\n",
            "  651.25630753 1000.        ]\n",
            "31. Initial State Value: 183.52874986975073\n",
            "    Policy: (0, 0, 0, 0, 1)\n",
            "    All Values: [   0.          -29.21269435   37.79510039  183.52874987  393.67446252\n",
            "  656.52095439 1000.        ]\n",
            "32. Initial State Value: 163.70370694432827\n",
            "    Policy: (0, 0, 0, 1, 1)\n",
            "    All Values: [   0.          -37.18222983   23.30503585  163.70370694  369.48443719\n",
            "  643.21644045 1000.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "64AmzV0rJuOD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2\n",
        "\n",
        "The action values are shown below, where each pair of values represents the values at a state, the first (index 0) being for taking a high effort action and the second (index 1) being for taking a low effort action.  Based on this, the target policy does not appear optimal because at all non-terminal states except for when you are 2 wins ahead, they are playing high effort, and when 2 wins ahead, they are playing low effot.  However, even just at the first state (s = -2) the value of low effort (112.74) is higher than the value of high effort (56.73) which is what we take given the policy.  Something similar happens at the fourth state (s = 1).  "
      ]
    },
    {
      "metadata": {
        "id": "bWS79WjHFTZi",
        "colab_type": "code",
        "outputId": "3524db52-5661-4bc8-c919-f02f775a8243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "Q_sa = [[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
        "C_sa = [[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
        "        \n",
        "\n",
        "\n",
        "probs = [.55,.45]\n",
        "costs = [-50, -10]\n",
        "\n",
        "R = 1000\n",
        "\n",
        "\n",
        "\n",
        "episodes = 500000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def targetpolicy(s):\n",
        "    if s <= 1:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = 1\n",
        "    return action\n",
        "\n",
        "def behavepolicy():\n",
        "    return int(np.random.uniform() < .5)\n",
        "\n",
        "def r_update(sprime, a):\n",
        "    if sprime == 3 :\n",
        "        rout = 1000 + costs[a]\n",
        "    else :\n",
        "        rout = costs[a]\n",
        "    return rout\n",
        "\n",
        "def moving(prevstate, prob):\n",
        "    tempmove = int(np.random.uniform() < prob)\n",
        "    if tempmove == 0:\n",
        "        tempmove = -1\n",
        "    newstate = prevstate + tempmove\n",
        "    return newstate\n",
        "\n",
        "\n",
        "\n",
        "for i in range(episodes):\n",
        "    G = 0\n",
        "    W = 1\n",
        "\n",
        "    sprime = [0]\n",
        "    aprime = [behavepolicy()]\n",
        "    rprime = [0]\n",
        "    \n",
        "    t = 0\n",
        "    while True:\n",
        "        sprime.append(moving(sprime[t], probs[aprime[t]]))\n",
        "        rprime.append(r_update(sprime[(t+1)], aprime[t]))\n",
        "        if abs(sprime[t+1]) == 3:\n",
        "            break\n",
        "        \n",
        "\n",
        "        aprime.append(behavepolicy())\n",
        "        t += 1\n",
        "\n",
        "\n",
        "    step = len(sprime)-2\n",
        "    while True :\n",
        "        G += rprime[step+1]\n",
        "        C_sa[sprime[step]+2][aprime[step]] += W\n",
        "        Q_sa[sprime[step]+2][aprime[step]] += (W/C_sa[sprime[step]+2][aprime[step]])*(G - Q_sa[sprime[step]+2][aprime[step]])\n",
        "        targetAct = targetpolicy(sprime[step])\n",
        "        pi_sa = int(targetAct == aprime[step])\n",
        "        W = W*(pi_sa/.5)\n",
        "        if W == 0 or step <= 0:\n",
        "            break\n",
        "        step -= 1\n",
        "\n",
        "print(Q_sa)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[56.72557451955369, 112.73728546876116], [194.82414446706645, 150.01202307349183], [352.15796769137273, 288.24459868149296], [534.5021681991215, 562.1378879036548], [820.3342920106122, 788.4210841128363]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BlIv7AujJymS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.3\n",
        "\n",
        "The optimal policy computed using the off-policy Monte Carlo algorithm using a behavior policy that in each state selects either action equiprobably is shown below, along with the values.  This is 1,1,0,1,0 or low, low, high, low, high.  This is not the top ranked policy in part 1.1, but it is near the top and ranked #6.  While the optimal policy based on this algorithm might not match 1.1 exactly, it should be expected that it is not near the bottom, and that there is some agreement that the policy is good.  This seems to be the case.  This still holds that it's mostly low effort when close to losing, and mostly higher effort when closer to winning, although it does have the person playing low effort at state 1.  "
      ]
    },
    {
      "metadata": {
        "id": "h9PruVy-Byk6",
        "colab_type": "code",
        "outputId": "1d970c73-c72a-4540-9f03-446322504ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "Q_sa = [[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
        "C_sa = [[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
        "        \n",
        "\n",
        "probs = [.55,.45]\n",
        "costs = [-50, -10]\n",
        "\n",
        "R = 1000\n",
        "\n",
        "episodes = 500000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pi_s = np.argmax(Q_sa, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "def behavepolicy():\n",
        "    return int(np.random.uniform() < .5)\n",
        "\n",
        "def r_update(sprime, a):\n",
        "    if sprime == 3 :\n",
        "        rout = 1000 + costs[a]\n",
        "    else :\n",
        "        rout = costs[a]\n",
        "    return rout\n",
        "\n",
        "\n",
        "def moving(prevstate, prob):\n",
        "    tempmove = int(np.random.uniform() < prob)\n",
        "    if tempmove == 0:\n",
        "        tempmove = -1\n",
        "    newstate = prevstate + tempmove\n",
        "    return newstate\n",
        "\n",
        "\n",
        "\n",
        "for i in range(episodes):\n",
        "    G = 0\n",
        "    W = 1\n",
        "\n",
        "    sprime = [0]\n",
        "    aprime = [behavepolicy()]\n",
        "    rprime = [0]\n",
        "    \n",
        "    t = 0\n",
        "    while True:\n",
        "        sprime.append(moving(sprime[t], probs[aprime[t]]))\n",
        "        rprime.append(r_update(sprime[(t+1)], aprime[t]))\n",
        "        if abs(sprime[t+1]) == 3:\n",
        "            break\n",
        "        \n",
        "        aprime.append(behavepolicy())\n",
        "        t += 1\n",
        "\n",
        "    step = len(sprime)-2\n",
        "    while True :\n",
        "        G += rprime[step+1]\n",
        "        C_sa[sprime[step]+2][aprime[step]] += W\n",
        "        Q_sa[sprime[step]+2][aprime[step]] += (W/C_sa[sprime[step]+2][aprime[step]])*(G - Q_sa[sprime[step]+2][aprime[step]])\n",
        "        pi_s[sprime[step]+2] = np.argmax(Q_sa[sprime[step]+2])\n",
        "        if aprime[step] != pi_s[sprime[step]+2] or step <= 0:\n",
        "            break\n",
        "        W = W*(1/.5)\n",
        "        step -= 1\n",
        "\n",
        "print(Q_sa)\n",
        "print(pi_s)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[30.91307786325384, 36.54735983760822], [122.22101271793751, 163.2379935676167], [328.5416506785813, 323.74409497947244], [522.835876273051, 558.4393621519083], [783.7077636283753, 749.8518856333816]]\n",
            "[1 1 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ZVGSEzqJ2iS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "## 2.1\n",
        "\n",
        "The Q-learning algorithm using an $\\epsilon$-greedy policy with $\\epsilon = 0.1$ is implemented below to solve the problem.   The values are shown below, and the optimal policy that corresponds to these is 1,1,1,1,0 or low, low, low, low, high which is ranked #2 in part 1.1.  Again, this makes sense that they would both agree that a particular policy is one of the better policies, but not necessarily agree exactly the very optimal policy.  It also makes sense with the reasoning described in 1.1 of using low effort when close to losing and high effort when close to winning.  "
      ]
    },
    {
      "metadata": {
        "id": "ptENy8yfJkrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b0258345-c04b-4bcf-a59b-9fabdb60ce2f"
      },
      "cell_type": "code",
      "source": [
        "probs = [.55,.45]\n",
        "costs = [-50, -10]\n",
        "R = 1000\n",
        "\n",
        "Q_sa = [[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
        "\n",
        "\n",
        "episodes = 500000\n",
        "eta = 0.1\n",
        "epsilon = 0.1\n",
        "\n",
        "def moving(prevstate, prob):\n",
        "    tempmove = int(np.random.uniform() < prob)\n",
        "    if tempmove == 0:\n",
        "        tempmove = -1\n",
        "    newstate = prevstate + tempmove\n",
        "    return newstate\n",
        "\n",
        "def r_update(sprime, a):\n",
        "    if sprime == 3 :\n",
        "        rout = 1000 + costs[a]\n",
        "    else :\n",
        "        rout = costs[a]\n",
        "    return rout\n",
        "\n",
        "def Q_update(sprime):\n",
        "    if abs(sprime) == 3 :\n",
        "        Qout = 0\n",
        "    else:\n",
        "        Qout = max(Q_sa[sprime+2])\n",
        "    return Qout\n",
        "    \n",
        "\n",
        "def greedy(Q_s, epsilon):\n",
        "    if np.random.uniform() > epsilon:\n",
        "        achoice = np.argmax(Q_s)\n",
        "    else:\n",
        "        achoice = int(len(Q_s)*np.random.uniform())\n",
        "    return achoice\n",
        "    \n",
        "    \n",
        "for i in range(episodes):\n",
        "    s = 0\n",
        "    while True:\n",
        "        a = greedy(Q_sa[s], epsilon)\n",
        "        sprime = moving(s, probs[a])\n",
        "        r = r_update(sprime, a)\n",
        "        Q_sa[s+2][a] += eta*(r + Q_update(sprime) - Q_sa[s+2][a])\n",
        "        s = sprime\n",
        "        if abs(s) == 3:\n",
        "            break\n",
        "            \n",
        "print(Q_sa)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[105.71003390081543, 161.86976617685204], [289.4405522205924, 308.5391487633952], [455.90646004869, 524.383994850877], [648.8936616141898, 666.8805812306487], [804.9377275099197, 771.1552702557246]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yhDrWZE3J4P1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2\n",
        "\n",
        "The Sarsa algorithm using an $\\epsilon$-greedy policy with $\\epsilon = 0.1$ is implemented below to solve the problem.   The values are shown below, and the optimal policy that corresponds to these is 1,1,1,1,0 or low, low, low, low, high which is ranked #2 in part 1.1 and the same as 2.1.  Again, this makes sense that they would both agree that a particular policy is one of the better policies, but not necessarily agree exactly the very optimal policy.  It also makes sense with the reasoning described in 1.1 of using low effort when close to losing and high effort when close to winning.  "
      ]
    },
    {
      "metadata": {
        "id": "wXx6kR3LJk2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d547d6a2-1aa4-4565-8adb-9b878de89d48"
      },
      "cell_type": "code",
      "source": [
        "probs = [.55,.45]\n",
        "costs = [-50, -10]\n",
        "R = 1000\n",
        "\n",
        "Q_sa = [[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
        "\n",
        "\n",
        "episodes = 500000\n",
        "eta = 0.1\n",
        "epsilon = 0.1\n",
        "\n",
        "\n",
        "def moving(prevstate, prob):\n",
        "    tempmove = int(np.random.uniform() < prob)\n",
        "    if tempmove == 0:\n",
        "        tempmove = -1\n",
        "    newstate = prevstate + tempmove\n",
        "    return newstate\n",
        "\n",
        "def r_update(sprime, a):\n",
        "    if sprime == 3 :\n",
        "        rout = 1000 + costs[a]\n",
        "    else :\n",
        "        rout = costs[a]\n",
        "    return rout\n",
        "\n",
        "def Q_update(sprime, aprime):\n",
        "    if abs(sprime) == 3 :\n",
        "        return 0\n",
        "    else:\n",
        "        return Q_sa[sprime+2][aprime]\n",
        "\n",
        "def greedy(Q_s, epsilon):\n",
        "    if np.random.uniform() > epsilon:\n",
        "        achoice = np.argmax(Q_s)\n",
        "    else:\n",
        "        achoice = int(len(Q_s)*np.random.uniform())\n",
        "    return achoice\n",
        "    \n",
        "for i in range(episodes):\n",
        "    s = 0\n",
        "    a = greedy(Q_sa[s], epsilon)\n",
        "    while True:\n",
        "        sprime = moving(s, probs[a])\n",
        "        r = r_update(sprime, a)\n",
        "        aprime = greedy(Q_sa[sprime], epsilon)\n",
        "        Q_sa[s+2][a] += eta*(r + Q_update(sprime, aprime) - Q_sa[s+2][a])\n",
        "        s = sprime\n",
        "        a = aprime\n",
        "        if abs(s) == 3:\n",
        "            break\n",
        "        \n",
        "        \n",
        "print(Q_sa)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-38.051204373148366, -2.2760077168270922], [-19.915888120452923, 64.6000606553439], [121.6879963611231, 153.22251289523803], [353.2439199282871, 419.2019532049044], [768.7397224017661, 676.9025264709442]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8mKNV897J55v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3\n",
        "\n",
        "Using the online TD($\\lambda$)-algorithm, the values of a random policy are shown below.  While the values are not as high as the top ranked deterministic policy from 1.1, they are still on par with the values of a lot of the other deterministic policies which were evaluated.  "
      ]
    },
    {
      "metadata": {
        "id": "yl_wdiQktmzM",
        "colab_type": "code",
        "outputId": "7a869a32-f607-4108-c8a8-594f15975933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "probs = [.55,.45]\n",
        "costs = [-50, -10]\n",
        "\n",
        "V = [0,0,0,0,0]\n",
        "e = [0,0,0,0,0]\n",
        "episodes = 500000\n",
        "lamb = 0.9\n",
        "eta = 0.0001\n",
        "\n",
        "R = 1000\n",
        "\n",
        "def r_update(sprime, a):\n",
        "    if sprime == 3 :\n",
        "        rout = 1000 + costs[a]\n",
        "    else :\n",
        "        rout = costs[a]\n",
        "    return rout\n",
        "\n",
        "def moving(prevstate, prob):\n",
        "    tempmove = int(np.random.uniform() < prob)\n",
        "    if tempmove == 0:\n",
        "        tempmove = -1\n",
        "    newstate = prevstate + tempmove\n",
        "    return newstate\n",
        "\n",
        "\n",
        "def deltacalc(r, sprime, s):\n",
        "    if abs(sprime) == 3 :\n",
        "        delta = r - V[s+2]\n",
        "#     elif sprime == 3:\n",
        "#         delta = r + 1000 - V[s+2]\n",
        "    else:\n",
        "        delta = r + V[sprime+2] - V[s+2]\n",
        "    return delta\n",
        "\n",
        "for i in range(episodes):\n",
        "    s = 0\n",
        "    while True:\n",
        "        a = int(np.random.uniform() > .5)\n",
        "        sprime = moving(s, probs[a])\n",
        "        r = r_update(sprime, a)\n",
        "        \n",
        "        delta = deltacalc(r, sprime, s)\n",
        "        e[s+2] += 1\n",
        "        for j in range(len(V)):\n",
        "            V[j] += eta*e[j]*delta\n",
        "            e[j] = lamb*e[j]\n",
        "        s = sprime\n",
        "        if abs(s) == 3:\n",
        "            break\n",
        "            \n",
        "print(V)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10.342868347566643, 92.82976953549976, 234.46380568023812, 433.8881578560655, 690.5138976664559]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tD_uoeLJfMo5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 3\n",
        "\n",
        "The problem is solved using the SARSA($\\lambda$) algorithm using an $\\epsilon$-greedy policy with $\\epsilon = 0.1$.  The estimated action values for the different states and policy are shown below where each block of 11 rows represents states with differing numbers of wins (-2 through 2 in order), each row within that represents a different energy level state (0 through 10), and each column is the action taken (0 being high or 1 being low).  In almost every case with a high enough energy (all but the first row of each section), the high effort has a higher value.  If the energy is too low to use high energy, then the value of high energy is 0 as expected, since high energy cannot be used.  As the number of wins goes up and the energy level increases, the values in general increase for both actions as expected since the chances of winning increase.  "
      ]
    },
    {
      "metadata": {
        "id": "eLSqTZcGETdu",
        "colab_type": "code",
        "outputId": "04acf6b3-c8ca-4e3d-f6db-1183c2f12446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1870
        }
      },
      "cell_type": "code",
      "source": [
        "probs = [.55,.45]\n",
        "costs = 1\n",
        "B = 10\n",
        "R = 1000\n",
        "a = 2\n",
        "p = .2\n",
        "\n",
        "Q_sa = np.array([[[0.0]*2]*11]*5) \n",
        "e_sa = np.array([[[0.0]*2]*11]*5) \n",
        "\n",
        "\n",
        "\n",
        "episodes = 500000\n",
        "eta = 0.001\n",
        "lamb = 0.9\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "\n",
        "\n",
        "def Q_update(sprime, ce, aprime):\n",
        "    if sprime == 3 :\n",
        "        Qout= 1000\n",
        "    elif sprime == -3:\n",
        "        Qout= 0\n",
        "    else:\n",
        "        Qout= Q_sa[sprime+2][ce][aprime]\n",
        "    return Qout\n",
        "\n",
        "def moving(prevstate, prob):\n",
        "    tempmove = int(np.random.uniform() < prob)\n",
        "    if tempmove == 0:\n",
        "        tempmove = -1\n",
        "    newstate = prevstate + tempmove\n",
        "    return newstate\n",
        "\n",
        "def r_update(newstate, Action):\n",
        "    reward = 0\n",
        "    if newstate == 3:\n",
        "        reward = R\n",
        "    return reward\n",
        "\n",
        "def greedy(Q_s, epsilon):\n",
        "    if np.random.uniform() > epsilon:\n",
        "        achoice = np.argmax(Q_s)\n",
        "    else:\n",
        "        achoice = int(len(Q_s)*np.random.uniform())\n",
        "    return achoice\n",
        "\n",
        "def updateEnergy(ce, a):\n",
        "    if a == 0:\n",
        "        ce -= costs\n",
        "        ce = max(0,ce)\n",
        "    if np.random.uniform() < p:\n",
        "        ce += a\n",
        "        ce = min(10,ce)\n",
        "    return ce\n",
        "    \n",
        "\n",
        "for i in range(episodes):\n",
        "    if (i+1) % 10000 == 0:\n",
        "        print(\"Episode: \" + str(i+1))\n",
        "    s = 0\n",
        "    ce = B\n",
        "    a = int(np.random.uniform() > .5)\n",
        "    while True:\n",
        "        ceprime = updateEnergy(ce, a)\n",
        "        sprime = moving(s, probs[a])\n",
        "        r = r_update(sprime, a)\n",
        "        if abs(sprime) < 3:\n",
        "            if ceprime < costs:\n",
        "                aprime = 1\n",
        "            else:\n",
        "                aprime = greedy(Q_sa[sprime + 2][ceprime], epsilon)\n",
        "        delta = gamma*Q_update(sprime,ceprime, aprime) - Q_sa[s + 2][ce][a]\n",
        "        e_sa[s+2][ce][a] += 1\n",
        "        for j in range(len(Q_sa)):\n",
        "            for k in range(len(Q_sa[0])):\n",
        "                for l in range(len(Q_sa[0][0])):\n",
        "                    Q_sa[j][k][l] += eta*e_sa[j][k][l]*delta\n",
        "                    e_sa[j][k][l] = gamma*lamb*e_sa[j][k][l]\n",
        "        s = sprime\n",
        "        a = aprime\n",
        "        ce = ceprime\n",
        "        if abs(sprime) == 3: \n",
        "            break\n",
        "        \n",
        "print(Q_sa)\n",
        "                \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 10000\n",
            "Episode: 20000\n",
            "Episode: 30000\n",
            "Episode: 40000\n",
            "Episode: 50000\n",
            "Episode: 60000\n",
            "Episode: 70000\n",
            "Episode: 80000\n",
            "Episode: 90000\n",
            "Episode: 100000\n",
            "Episode: 110000\n",
            "Episode: 120000\n",
            "Episode: 130000\n",
            "Episode: 140000\n",
            "Episode: 150000\n",
            "Episode: 160000\n",
            "Episode: 170000\n",
            "Episode: 180000\n",
            "Episode: 190000\n",
            "Episode: 200000\n",
            "Episode: 210000\n",
            "Episode: 220000\n",
            "Episode: 230000\n",
            "Episode: 240000\n",
            "Episode: 250000\n",
            "Episode: 260000\n",
            "Episode: 270000\n",
            "Episode: 280000\n",
            "Episode: 290000\n",
            "Episode: 300000\n",
            "Episode: 310000\n",
            "Episode: 320000\n",
            "Episode: 330000\n",
            "Episode: 340000\n",
            "Episode: 350000\n",
            "Episode: 360000\n",
            "Episode: 370000\n",
            "Episode: 380000\n",
            "Episode: 390000\n",
            "Episode: 400000\n",
            "Episode: 410000\n",
            "Episode: 420000\n",
            "Episode: 430000\n",
            "Episode: 440000\n",
            "Episode: 450000\n",
            "Episode: 460000\n",
            "Episode: 470000\n",
            "Episode: 480000\n",
            "Episode: 490000\n",
            "Episode: 500000\n",
            "[[[  0.          50.98780981]\n",
            "  [ 40.32924522  45.23115147]\n",
            "  [ 49.36157636  46.08765032]\n",
            "  [ 35.52594767  59.5492578 ]\n",
            "  [ 64.72126303  37.22770197]\n",
            "  [ 68.61550571  45.57748227]\n",
            "  [ 73.13548048  48.74961866]\n",
            "  [ 75.226329    57.44578227]\n",
            "  [ 76.52675046  62.74263313]\n",
            "  [ 78.05946997  63.21495273]\n",
            "  [ 81.43484721  12.10736537]]\n",
            "\n",
            " [[  0.         104.33479156]\n",
            "  [101.58946445 113.74361459]\n",
            "  [119.30819456  47.78652887]\n",
            "  [131.65549006 118.15740924]\n",
            "  [144.32637661 120.51609875]\n",
            "  [148.3771979  120.17174762]\n",
            "  [164.18739153 139.22937953]\n",
            "  [159.75604502 137.90629297]\n",
            "  [165.30075944 147.54143522]\n",
            "  [162.82503351 147.10272989]\n",
            "  [172.80066383 156.20376146]]\n",
            "\n",
            " [[  0.         179.71425675]\n",
            "  [194.18027302 211.91075504]\n",
            "  [226.21844626 207.88363978]\n",
            "  [244.15623666 215.91846639]\n",
            "  [247.68624206 233.06981796]\n",
            "  [280.00406171 228.46881478]\n",
            "  [268.81825248 231.35023466]\n",
            "  [274.59761031 250.84077002]\n",
            "  [276.79880303 246.87063486]\n",
            "  [287.10739719 243.97572091]\n",
            "  [271.80115316 268.01636507]]\n",
            "\n",
            " [[  0.         329.11998905]\n",
            "  [365.0639754  340.61920294]\n",
            "  [403.24088591 231.3139932 ]\n",
            "  [401.40457067 366.08625985]\n",
            "  [442.55873326 355.24487111]\n",
            "  [418.24643857 380.47770556]\n",
            "  [411.48526823 401.82278617]\n",
            "  [433.8623663  403.10413545]\n",
            "  [439.22629341 406.7259848 ]\n",
            "  [425.99536035 409.19686653]\n",
            "  [436.53166526 410.34826477]]\n",
            "\n",
            " [[  0.         579.62606185]\n",
            "  [643.7528311  524.8256481 ]\n",
            "  [631.4914149  547.40029991]\n",
            "  [673.32693499 449.73028905]\n",
            "  [642.83236314 566.55316673]\n",
            "  [591.87296573 612.59432176]\n",
            "  [661.59150853 576.44566896]\n",
            "  [656.3619313  586.6921673 ]\n",
            "  [663.02093314 614.46410849]\n",
            "  [666.74159773 592.17756631]\n",
            "  [646.638546   110.60239971]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MbD1KD6PkGJU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem 4\n",
        "\n",
        "$$P(Y\\ loses\\ 1\\ coin) = P(X\\ loses\\ 1\\ coin) = 0.5$$\n",
        "\n",
        "Each round in assumed to be independent, so:\n",
        "\n",
        "$$P(Round\\ 1\\ Outcome\\ \\cap \\ Round\\ 2\\ Outcome) =  P(Round\\ 1\\ Outcome)\\times P(Round\\ 2\\ Outcome)$$\n",
        "\n",
        "## 4.1\n",
        "\n",
        "The winning probability for player X given that $x = 1$ is $0.5^y$.  The derivation can be seen below:  \n",
        "\n",
        "\n",
        "$$P(X\\ wins) = P(Y\\ loses\\ all\\ y\\ coins) = P(Y\\ loses\\ round\\ 1)\\times P(Y\\ loses\\ round\\ 2)\\times\\ ...\\ \\times P(Y\\ loses\\ round\\ y) = \\prod^y_{i = 1}0.5 = 0.5^y$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 4.2\n",
        "\n",
        "The winning probability for player X given that $y = 1$ is $1 - 0.5^x$.  The derivation can be seen below:  \n",
        "\n",
        "$$P(X\\ wins) = 1 - P(Y\\ wins) = 1- P(X\\ loses\\ all\\ x\\ coins) = 1- 0.5^x$$\n",
        "\n",
        "\n",
        "## 4.3 \n",
        "\n",
        "The winning probability for player X for each value of x and y is given by:\n",
        " $$P(X\\ wins) = \\sum _{i = 0}^{x-1} { y - 1 + i \\choose i } \\times (0.5) ^ {(y + i)}$$\n",
        " \n",
        " The derivation can be seen below:  \n",
        "\n",
        "<br>\n",
        "\n",
        "$$P(X\\ wins) = P(Y\\ loses\\ (y - 1)\\ coins; X\\ loses \\leq (x-1)\\ coins; Y\\ loses\\ last\\ coin\\ on\\ last\\ round)$$ \\\\\n",
        "\n",
        "\n",
        "\n",
        "$$P(X\\ wins) = P(Y\\ loses\\ (y - 1); X\\ loses\\ 0; Y\\ loses\\ last) + P(Y\\ loses\\ (y - 1); X\\ loses\\ 1; Y\\ loses\\ last) + P(Y\\ loses\\ (y - 1); X\\ loses\\ 2; Y\\ loses\\ last)\\ +\\ ...\\ +\\ P(Y\\ loses\\ (y - 1); X\\ loses\\ (x - 1); Y\\ loses\\ last)$$ \\\n",
        "<br>\n",
        "\n",
        "$$P(X\\ wins) = (.5)^{(y-1)}{y-1  \\choose 0}(.5) + (.5)^{(y - 1 + 1)}{y -1 + 1 \\choose 1}(.5) + (.5)^{(y - 1 + 2)}{y -1 + 2 \\choose 2}(.5)\\ +\\ ...\\ +\\ .5^{(y - 1 + x - 1)}{y-1 + x-1 \\choose x-1 }(.5) $$ \\\\\n",
        "\n",
        "<br>\n",
        "\n",
        " $$P(X\\ wins) = \\sum _{i = 0}^{x-1} { y - 1 + i \\choose i } \\times (0.5) ^ {(y + i)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}